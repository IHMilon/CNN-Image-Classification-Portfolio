{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOljMBNbbYFxhXHgZ1BFFxg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IHMilon/CNN-Image-Classification-Portfolio/blob/main/VGG11_CIFAR10/CNN_SHIFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuWbEJexxB0s"
      },
      "outputs": [],
      "source": [
        "!pip install torchprofile 1>/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from collections import OrderedDict, defaultdict\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.optim import *\n",
        "from torch.optim.lr_scheduler import *\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torchprofile import profile_macs\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.optim.lr_scheduler import OneCycleLR"
      ],
      "metadata": {
        "id": "Uc1lluyNxPw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_value = 0\n",
        "torch.manual_seed(seed_value)\n",
        "np.random.seed(seed_value)\n",
        "random.seed(seed_value)\n",
        "torch.cuda.manual_seed_all(seed_value)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "pNVmw3gJxTbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),  # data augmentation\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    ])"
      ],
      "metadata": {
        "id": "4XoF5RIoxWEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=train_transform\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=test_transform\n",
        ")"
      ],
      "metadata": {
        "id": "-WWe2oSZxlsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True,\n",
        "     pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=batch_size, shuffle=False,\n",
        "     pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "hPeoEAGSxuym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = next(iter(train_loader))\n",
        "print( f\"Image batch shape: {images.shape}\")  # (B, C, H, W)\n",
        "print( f\"Label batch shape: {labels.shape}\")  # (B,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbpLfac7xvkV",
        "outputId": "4d40834d-74bc-4b97-98bf-c63f4c86aa33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image batch shape: torch.Size([64, 3, 32, 32])\n",
            "Label batch shape: torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CPSMBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Co-Prime Shift Mixer (CPSM) block.\n",
        "    Can be used like a Transformer block: input -> CPSM -> output (residual)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels=None, shifts=None, use_residual=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            in_channels (int): Number of input channels\n",
        "            out_channels (int): Number of output channels (default = in_channels)\n",
        "            shifts (list of tuple): List of (dy, dx) shifts to apply. Default: 4 shifts\n",
        "            use_residual (bool): Whether to use residual connection\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels or in_channels\n",
        "        self.use_residual = use_residual\n",
        "\n",
        "        # Default shifts: 4 co-prime style shifts\n",
        "        if shifts is None:\n",
        "            self.shifts = [(0,0), (0,1), (1,0), (1,1)]\n",
        "        else:\n",
        "            self.shifts = shifts\n",
        "\n",
        "        # 1x1 mixer to combine stacked channels\n",
        "        self.mixer = nn.Conv2d(\n",
        "            in_channels * len(self.shifts),\n",
        "            self.out_channels,\n",
        "            kernel_size=1,\n",
        "            bias=False\n",
        "        )\n",
        "        self.bn = nn.BatchNorm2d(self.out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Residual scaling factor (learnable)\n",
        "        self.alpha = nn.Parameter(torch.zeros(1))  # better\n",
        "\n",
        "        # Optional projection if in/out channels differ\n",
        "        if self.use_residual and in_channels != self.out_channels:\n",
        "            self.res_proj = nn.Conv2d(in_channels, self.out_channels, kernel_size=1, bias=False)\n",
        "        else:\n",
        "            self.res_proj = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Step 1: Apply shifts\n",
        "        shifted_maps = []\n",
        "        for dy, dx in self.shifts:\n",
        "            shifted = torch.roll(x, shifts=(dy, dx), dims=(2,3))  # circular roll\n",
        "            shifted_maps.append(shifted)\n",
        "\n",
        "        # Step 2: Stack shifted maps along channel dimension\n",
        "        stacked = torch.cat(shifted_maps, dim=1)  # B x (C*N) x H x W\n",
        "\n",
        "        # Step 3: Mix channels with 1x1 conv\n",
        "        mixed = self.mixer(stacked)\n",
        "        mixed = self.bn(mixed)\n",
        "        mixed = self.relu(mixed)\n",
        "\n",
        "        # Step 4: Residual connection\n",
        "        if self.use_residual:\n",
        "            residual = self.res_proj(x) if self.res_proj is not None else x\n",
        "            out = residual + self.alpha * mixed\n",
        "        else:\n",
        "            out = mixed\n",
        "        return out\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "Wu_Xofs5zRNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CPSMNetCIFAR10(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        # Stem\n",
        "        self.stem = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Stage 1\n",
        "        self.stage1 = nn.Sequential(\n",
        "            CPSMBlock(64, 128, shifts=[(0,0),(1,0),(0,1),(1,1)]),\n",
        "            CPSMBlock(128, 128, shifts=[(0,0),(2,0),(0,2),(1,1)])\n",
        "        )\n",
        "        self.down1 = nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "\n",
        "        # Stage 2\n",
        "        self.stage2 = nn.Sequential(\n",
        "            CPSMBlock(128, 256, shifts=[(0,0),(1,0),(0,2),(2,1)]),\n",
        "            CPSMBlock(256, 256, shifts=[(0,0),(3,0),(0,3),(2,2)])\n",
        "        )\n",
        "        self.down2 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "\n",
        "        # Stage 3\n",
        "        self.stage3 = nn.Sequential(\n",
        "            CPSMBlock(256, 256, shifts=[(0,0),(1,0),(0,4),(3,2)]),\n",
        "            CPSMBlock(256, 256, shifts=[(0,0),(4,0),(0,5),(3,3)])\n",
        "        )\n",
        "\n",
        "        # Classifier Head (256-dim input now)\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(256, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.stem(x)))\n",
        "        x = self.stage1(x)\n",
        "        x = self.down1(x)\n",
        "        x = self.stage2(x)\n",
        "        x = self.down2(x)\n",
        "        x = self.stage3(x)\n",
        "        x = self.pool(x).flatten(1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "kUhuXkg18Je9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=CPSMNetCIFAR10(num_classes=10)\n",
        "model=model.to(device)"
      ],
      "metadata": {
        "id": "SpfSgzUq0BVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_params = 0\n",
        "for param in model.parameters():\n",
        "  if param.requires_grad:\n",
        "    num_params += param.numel()\n",
        "print(\"#Params:\", num_params/1000000,\"Million\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3O48HPr0P-E",
        "outputId": "740cda3d-53c7-4a76-f72c-b0e87da3a028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Params: 1.867344 Million\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()  # Set the model to evaluation mode\n",
        "num_macs = profile_macs(model, torch.zeros(1, 3, 32, 32).cuda())\n",
        "print(\"#MACs:\", num_macs/1000000,\"Millions\")"
      ],
      "metadata": {
        "id": "j_oAActF1pVc",
        "outputId": "12d299a6-e9f7-479e-a3c2-ef858d36b262",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#MACs: 329.911296 Millions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::roll\". Skipped.\n",
            "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "LR = 0.001\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(model.parameters(), lr=LR, weight_decay=1e-2)\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)  # decay lr every 5 epochs"
      ],
      "metadata": {
        "id": "T7F3eE_91txP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        train_loss, train_correct, train_total = 0, 0, 0\n",
        "        for images, labels in tqdm(train_loader, desc=f\"[Train] Epoch {epoch+1}/{EPOCHS}\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            predicted = outputs.argmax(dim=1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_acc = 100. * train_correct / train_total\n",
        "\n",
        "        model.eval()\n",
        "        val_loss, val_correct, val_total = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in tqdm(test_loader, desc=f\"[Val] Epoch {epoch+1}/{EPOCHS}\"):\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                predicted = outputs.argmax(dim=1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        val_acc = 100. * val_correct / val_total\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "        print(f\"  Train Loss: {train_loss/len(train_loader):.4f} | Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"  Val   Loss: {val_loss/len(test_loader):.4f} | Val Acc: {val_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "PMdy9U5q2ZyW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}